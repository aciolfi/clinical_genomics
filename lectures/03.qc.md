---
layout: page
title: Quality control
summary: "Quality control of NGS data"
sidebar_link: true
---

Before data analysis, it is crucial to check the quality of the data at different levels of the analysis workflow:
 1. Raw sequences in FASTQ format
 1. BAM files containing the sequences aligned to the reference genome
 1. Final list of variants if VCF format

In this training we will use several tools available at [https://usegalaxy.eu](https://usegalaxy.eu)
for checking the quality of data at each step of the analysis, as well as to evaluate the per-base coverage depth at specific genomic intervals

![Quality control overview]({{site.url}}{{site.baseurl}}/images/qc_overview.png)
**Software for quality control**, at each step of the analysis workflow

### Quality control of FASTQ files

We'll use the **FastQC** software, with the FASTQ files available in the Shared Data Libraries: *HighQuality_Reads.fastq* and 
*LowQuality_Reads.fastq*


*FastQC* is relatively easy to use. A detailed help cab be found the [help manual](http://www.bioinformatics.babraham.ac.uk/projects/fastqc/Help/). 

 - Copy the FASTQ file in your history from the Galaxy Shared Data Libraries. Help needed? Follow the [Galaxy at a Glance]({{site.url}}{{site.baseurl}}/lectures/02.galaxy.html) tutorial
 - Select *FastQC* from the left panel
 - Select the FASTQ file as input
 - Click *Execute*

The output of FASTQC consists of multiple modules analysing a specific aspect of the quality of the data.

The names of the modules are preceded by an icon that reflects the quality of the data. The icon indicates whether the results of the module are:
 - normal (green)
 - slightly abnormal (orange)
 - very unusual (red)
 
These evaluations must be taken in the context of what you are expecting from your dataset. For FASTQC a *normal* includes random sequences with high diversity. If your experiment generates libraries biased in particular ways (e.g. low complexity libraries) you should consider the report with attention. In general, you should concentrate on the icons different from green and try to understand the reasons for this behaviour.

### Analysis modules in FASTQC:
#### Sequence quality by position.

Phred scores represent base call quality. The higher the score the more reliable the base call. Often the quality of reads decreases over the length of the read. Therefore, it is common practice to plot the distribution of the Phred scores at each position using boxplots.
    
The average Phred score is depicted by the blue line, the median Phred score by the red line. The yellow boxes contain 50% of all Phred scores. The background of the graph divides the plot in three regions: good quality (green), reasonable quality (orange), poor quality (red; Phred score < 20).

![High Quality reads]({{site.url}}{{site.baseurl}}/images/fastqc_sequence_quality.png)
Example of datasets with high (left) and low (right) sequence quality


#### Check overall sequence quality
Calculates the average Phred score of each read and show a cumulative plot of the average qualities of all the reads.

#### Check quality per tile
This plot is specific for Illumina flowcells which are divided into tiles. To check whether there is a loss in quality associated with a specific area of the flowcell, FASTQC plots the average quality scores for each tile across all positions in the reads. 

A good plot should be blue on all the area. Deviations from blue are positions where the quality was different from the average. Failures could be due to bubbles through the flowcells, presence of debris or, Illumina not patterned flow-cell, high density of clusters due to overlad. 
Failures affecting small areas of the flow-cell can be safely ignored.

#### Check sequence duplication
Due to random fragmentation, in a diverse library generated by shearing most fragments will occur only once. High level of duplication may indicate enrichment bias such as library contamination or PCR over-amplification.

The plot shows the relative number of reads with different degrees of duplication.
 - The red line represents the number of **distinct** sequences that are duplicated, where the percentage is relative to the total number of distinct sequences
 - The blue line represents the counts of all the duplicated sequences, where the percentage is computed relative to the total number of reads

Most sequences should fall in the left part of the plot (low sequence duplication level).
Contaminants are visible as peaks in the blue line (high proportion of original sequences) but should be absent in the red line as they contribute as a minor fraction of the deduplicated dataset. If peaks are still present in the red line after deduplication, you may have a large number of different duplicated sequences which might indicate contamination.

#### Check per base sequence content
In presence of random sequences, the fraction of A, C, G and T should be the same on each position, producing four straight lines on this plot, one for each nucleotide. However you will often see several peaks for the first positions. This bias in the first positions arises from the protocol used to generate RNA-Seq libraries, either transposase based or priming with random examers. The bias is not due to a single sequence, but depends on different K-mers at the 5' end of the reads, therefore cannot be removed by trimming a specific sequence from the 5' end.
However, the presence of such bias, if limited to the first bases, do not affect downstream analyses.

#### Over-represented sequences

This module checks for the presence of specific sequences which are over-represented in your library, i.e. reads that make a substantial fraction (&gt; 0.1%) of the total number of reads. In this module only the first 50bp are considered by FASTQC. Each overrepresented sequence is compared to a list of common contaminants to try to identify it.

You can see which position of the read is greatly affected (plot), and check the list of possible contaminants in the table with the list of sequences which make &gt; 0.1% of the total reads.

For DNA-Seq ideally you shouldn't see any sequence in the table, even if a small fraction of adapter sequences can be observed, which can be easily removed by adding a step for adapter removal in the analysis pipeline. For RNA-Seq, highly abundant transcripts could be tracked as overrepresented sequences.

The presence of adapter sequences impacts the rate and speed of alignment.

## Computation of per-base coverage depth at specific genomic intervals

Commercial next-generation sequencing platform usually provide users with analysis programs that include tools for the identification of low coverage regions (for instance, target regions that have a coverage depth lower than 20x).

The present tutorial is aimed to show how to perform a custom coverage analysis of NGS experiment(s) by using tools that are available in Galaxy.
 
Starting material:
* Alignment (*bam*) file(s) on which you want to perform the coverage evaluation.
* A reference *bed* file listing the genomic regions for which you want to obtain coverage data. If you performed a targeted sequencing experiment by using commercial kits (either custom or from the catalogue), you should already have obtained a *bed* file listing the target regions: it should be the file you want to use.

>#### NOTE: BED format specifications 
>
>**BED files** are tab-delimited files with one line for each genomic region.
>The first 3 fields (columns) are required:
>1. chromosome; 
>2. the starting position;
>3. the ending position.
>
>Further columns may be present but are optional.
>
>Further details may be found here: [UCSC BED format specifications](https://genome.ucsc.edu/FAQ/FAQformat.html#format1)
>
>Please be aware that in BED files the description of genomic regions follows the “0-start, half-open” coordinate system. Further details may be found here: [the "0-based, half-open" UCSC Genome Browser Coordinate Counting Systems](http://genome.ucsc.edu/blog/the-ucsc-genome-browser-coordinate-counting-systems/). Practically speaking, it means that the starting position is diminished by 1 bp with respect to the actual position. For instance, the region "chr2 50149071 50149399" in a *bed* file corresponds to the genomic interval from position 50149072 (included) to position 50149399 (included) on chromosome 2 (when the first base of the chromosome is defined as position 1). If you want to indicate a single base position in a *bed* file, it should be written like this: "chr2 50149071 50149072".

### 1.Compute per base coverage depth with BEDtools
Before starting, you have to upload the files you need for the analysis, following standard Galaxy procedure. 

You may use files provided as examples with this tutorial and called `Panel_alignment.bam` and `Panel_Target_regions.bed`. 

Please check that uploaded file datatypes (formats) are properly recognized by selecting `edit attributes` (i.e. the pencil sign in correspondence of each file you can find in your history) (indicated by the red arrow in figure 1) and then the tab datatypes (the blue arrow in figure 1). If the datatype is wrong, select the correct one from the drop-down list and press the `change datatype` button (the green arrow n figure 1).

**Figure 1**
![Figure 1]({{site.url}}{{site.baseurl}}/images/cov_fig1.png)

Once ready, you can select the tool named `bedtools Compute both the depth and breadth of coverage` in the `Operate on Genomic Intervals` section (see the red arrow in figure 2).

1. Select the *bed* file listing the target regions as "file A" (blue arrow in figure 2) and the *bam* file(s) you want to analyze as "file B" (green arrow in figure 2) (they should be listed in the drop-down menu if they have the correct format). You may analyze one or more *bam* files in a single run. 
2. If you want to analyze two or more .bam files, you can further choose if you want all the results in a single file or one output file per input "file B" by selecting the desired option under the `Combined or separate output files` menu.
3. Select "Yes" for the option `Report the depth at each position in each A feature` (yellow arrow in figure 2) and check that all the other options are set to "No".
4. Star (Execute) the analysis.

**Figure 2**
![Figure 2]({{site.url}}{{site.baseurl}}/images/cov_fig2.png)

Output file, which will be called "coverage_depth.bed" from now on, will contain all the fields of the original target_regions.bed file plus two further columns:
1. the first value indicates a specific base within the reported genomic interval. For instance, if the genomic interval described by the first 3 field is "chr2 50149071 50149072" and the first new field reports the number 1000, it means that the coverage value refers to nucleotide 50150071 (i.e.: 50149071 + 1000) on chromosome 2;
2. the second value indicates the depth of coverage for the defined base.

### 2.Sort files
Since entries in the "coverage_depth.bed" may not be in the desired order, you can sort it by genomic positions.
For this purpose you may want to use the `Sort` tool in the `Text Manipulation` section (check out the blue arrow in figure 3).

You may sequentially sort on different columns:
1. first you can sort by chromosome by selecting column 1 (green arrow in figure 3) in "ascending order" and selecting the "flavor" `Natural/Version sort (-V)`, which allows for sorting chromosomes in their "natural" order (with alphabetical order chr10 will be right after chr1, chr2 after chr19 and so on);
2. after inserting a new column section (red arrow in figure 3), you can sort by column 2 in "ascending order" with `Fast numeric sort (-n)`;
3. after inserting a further column section, you can sort by column 3 in "ascending order" with `Fast numeric sort (-n)`;
4. after inserting a final column section, you can sort by column 3 in "ascending order" with `Fast numeric sort (-n)`.

**Figure 3**
![Figure 3]({{site.url}}{{site.baseurl}}/images/cov_fig3.png)

The obtained output file, which will be called "sorted_coverage_depth.bed" from now on, will be sorted first by chromosome, then by starting position, by ending position and by the actual position of the specific base in the considered genomic interval.

### 3.Remove duplicate rows
If your file contains duplicated lines you may want to remove them for further processing.
For this purpose you can use the `Unique lines assuming sorted input file` tool in the `Text Manipulation` section.

### 4.Some manipulation of the *bed* file 
You may follow the following steps to manipulate the "sorted_coverage_depth.bed" and to obtain a *bed* file listing the exact base position to which each coverage value is referred.
For instance instead of having "chr2 50149071 50149399 NRXN1 1 2335" in the first row of your file, you will get "chr2 50149071 50149072 NRXN1 2335".

These steps will add further columns at the end of each line defining the base position with the “0-start, half-open” coordinate system.

1. Select the `Compute an expression on every row` tool in the `Text Manipulation` section (indicated by the blue arrow in figure 4);
2. add the following expression "c2+c5-1" to obtain the sum of the values in columns 2 and 5 minus 1 (it will be used as the new starting position in the final file);
3. select the file "sorted_coverage_depth.bed";
4. select "yes" to `round Results?` (green arrow in figure 4);
5. execute;
6. you can rename the output as "temp1_coverage_depth.bed";
7. select the `Compute an expression on every row` tool in the `Text Manipulation` section;
8. add the following expression "c2+c5" to obtain the sum of the values in columns 2 and 5 (it will be used as the new ending position in the final file). You can also use the expression ; 
9. select the file "temp1_coverage_depth.bed";
10. select "yes" to `round Results?`;
11. execute;
12. you can rename the output as "temp2_coverage_depth.bed";
13. select the `Table Compute` tool in the `Text Manipulation` section (indicated by the blue arrow in figure 5);
14. select the file "temp2_coverage_depth.bed";
15. select the option `Drop, keep or duplicate rows and columns` from the drop-down menu `Type of table operation` (indicated by the green arrow in figure 5);
16. fill the field `List of columns to select` with "1,7,8,4,6" (the red arrow in figure 5);
17. unselect all the other options;
18. execute;
19. set the output file datatype to *bed*;
20. you can rename the output as "final_coverage_depth.bed".

**Figure 4**
![Figure 4]({{site.url}}{{site.baseurl}}/images/cov_fig4.png)

**Figure 5**
![Figure 5]({{site.url}}{{site.baseurl}}/images/cov_fig5.png)

### 5.Select positions with low coverage depth
The following procedure can be used to obtain a *bed* file listing base positions with a coverage depth lower than a certain threshold (for instance 20x).

1. Select the `Filter` tool in the `Filter and Sort` section (indicated by the blue arrow in figure 6);
2. select the file "final_coverage_depth.bed";
2. add the following expression "c5<20" to filter all positions with a coverage depth lower than (green arrow in figure 6)("c5" stands for the fifth column, in this case reporting the coverage depth);
4. execute.

**Figure 6**
![Figure 6]({{site.url}}{{site.baseurl}}/images/cov_fig6.png)

The output file, which will be called "low_coverage_depth.bed" from now on, will only list all the positions with a depth lower than 20x.

### 6.Merge low coverage regions
If you want to merge the positions with low coverage depth in larger genomic intervals to be used for further analyses (i.e.: Sanger sequencing of regions not covered by your NGS experiment), you may want to use the `bedtools MergeBED` tool in the `Operate on Genomic Intervals` section (see the blue arrow in figure 7). 

1. Select the file "low_coverage_depth.bed";
2. set the maximum distance between features (i.e.: the different positions listed in your file) allowed for features to be merged (green arrow in figure 7): if it is 0 only overlapping and/or book-ended features are merged, while if it is set to 10 (or any other different positive integer of your choice), features at the maximum distance of 10 bases will be merged; 
3. if you want, you may apply some operations to certain columns to get further information in your output file. For instance you may:
	1. click on "Insert Applying operations to columns from merged intervals" (red arrow in figure 7), 
	2. specify the column on which you want to perform the operation (in this case column 5), 
	3. and select the operation from the drop-down list(in this case "Min", which calculates the minimum value of coverage depth among all the positions that will be merged in a single interval) (yellow arrow in figure 7);
4. you may add as many operations as you need. In this example we will also calculate the maximum value of coverage depth;
5. execute.	

**Figure 7**
![Figure 7]({{site.url}}{{site.baseurl}}/images/cov_fig7.png)

The output file will have the following fields (columns): chromosome, starting and ending positions of low coverage regions, the minimum and the maximum coverage depth in each region.

   ----------
   **Warning**: please be aware that the columns to use for calculations may be different compared to the example here considered, depending on the amount of columns of your *bed* files.
   
   **Warning**: please be aware that the Galaxy preview of your file shows a header row that does not properly define columns in your files (it is just a standard header for the UCSC bed format).
   
   ----------

>#### Final notes
>The procedures listed above are to be taken as examples of the possible operations that can be performed on bed files with bedtools (you may check out their website to get further information: [BEDtools](https://bedtools.readthedocs.io/en/latest/content/bedtools-suite.html)) ad text manipulation tools available on Galaxy.
>
>Furthermore, please be aware that the tool `bedtools Compute both the depth and breadth of coverage` does not perform any filtering based on read quality: if your are interested in that aspect you may want to rely on different tools. 
